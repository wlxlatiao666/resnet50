W1128 18:56:58.255000 39080 site-packages/torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.
W1128 18:56:58.263000 39080 site-packages/torch/distributed/run.py:803] 
W1128 18:56:58.263000 39080 site-packages/torch/distributed/run.py:803] *****************************************
W1128 18:56:58.263000 39080 site-packages/torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1128 18:56:58.263000 39080 site-packages/torch/distributed/run.py:803] *****************************************
=> creating model 'resnet50'
using mps, this will be slow
[rank3]: Traceback (most recent call last):
[rank3]:   File "/Users/weilongxuan/codes/resnet50/train.py", line 427, in <module>
[rank3]:     main()
[rank3]:   File "/Users/weilongxuan/codes/resnet50/train.py", line 113, in main
[rank3]:     main_worker(args.gpu, ngpus_per_node, args)
[rank3]:   File "/Users/weilongxuan/codes/resnet50/train.py", line 157, in main_worker
[rank3]:     model = torch.nn.parallel.DistributedDataParallel(model)
[rank3]:   File "/opt/miniconda3/lib/python3.13/site-packages/torch/nn/parallel/distributed.py", line 858, in __init__
[rank3]:     _verify_param_shape_across_processes(self.process_group, parameters)
[rank3]:   File "/opt/miniconda3/lib/python3.13/site-packages/torch/distributed/utils.py", line 281, in _verify_param_shape_across_processes
[rank3]:     return dist._verify_params_across_processes(process_group, tensors, logger)
[rank3]: NotImplementedError: The operator 'c10d::allgather_' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 5811a8d7da873dd699ff6687092c225caffcf1bb. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.
[rank0]: Traceback (most recent call last):
[rank0]:   File "/Users/weilongxuan/codes/resnet50/train.py", line 427, in <module>
[rank0]:     main()
[rank0]:     ~~~~^^
[rank0]:   File "/Users/weilongxuan/codes/resnet50/train.py", line 113, in main
[rank0]:     main_worker(args.gpu, ngpus_per_node, args)
[rank0]:     ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/Users/weilongxuan/codes/resnet50/train.py", line 157, in main_worker
[rank0]:     model = torch.nn.parallel.DistributedDataParallel(model)
[rank0]:   File "/opt/miniconda3/lib/python3.13/site-packages/torch/nn/parallel/distributed.py", line 858, in __init__
[rank0]:     _verify_param_shape_across_processes(self.process_group, parameters)
[rank0]:     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/miniconda3/lib/python3.13/site-packages/torch/distributed/utils.py", line 281, in _verify_param_shape_across_processes
[rank0]:     return dist._verify_params_across_processes(process_group, tensors, logger)
[rank0]:            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]: NotImplementedError: The operator 'c10d::allgather_' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 5811a8d7da873dd699ff6687092c225caffcf1bb. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.
[rank2]: Traceback (most recent call last):
[rank2]:   File "/Users/weilongxuan/codes/resnet50/train.py", line 427, in <module>
[rank2]:     main()
[rank2]:   File "/Users/weilongxuan/codes/resnet50/train.py", line 113, in main
[rank2]:     main_worker(args.gpu, ngpus_per_node, args)
[rank2]:   File "/Users/weilongxuan/codes/resnet50/train.py", line 157, in main_worker
[rank2]:     model = torch.nn.parallel.DistributedDataParallel(model)
[rank2]:   File "/opt/miniconda3/lib/python3.13/site-packages/torch/nn/parallel/distributed.py", line 858, in __init__
[rank2]:     _verify_param_shape_across_processes(self.process_group, parameters)
[rank2]:   File "/opt/miniconda3/lib/python3.13/site-packages/torch/distributed/utils.py", line 281, in _verify_param_shape_across_processes
[rank2]:     return dist._verify_params_across_processes(process_group, tensors, logger)
[rank2]: NotImplementedError: The operator 'c10d::allgather_' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 5811a8d7da873dd699ff6687092c225caffcf1bb. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.
[rank1]: Traceback (most recent call last):
[rank1]:   File "/Users/weilongxuan/codes/resnet50/train.py", line 427, in <module>
[rank1]:     main()
[rank1]:   File "/Users/weilongxuan/codes/resnet50/train.py", line 113, in main
[rank1]:     main_worker(args.gpu, ngpus_per_node, args)
[rank1]:   File "/Users/weilongxuan/codes/resnet50/train.py", line 157, in main_worker
[rank1]:     model = torch.nn.parallel.DistributedDataParallel(model)
[rank1]:   File "/opt/miniconda3/lib/python3.13/site-packages/torch/nn/parallel/distributed.py", line 858, in __init__
[rank1]:     _verify_param_shape_across_processes(self.process_group, parameters)
[rank1]:   File "/opt/miniconda3/lib/python3.13/site-packages/torch/distributed/utils.py", line 281, in _verify_param_shape_across_processes
[rank1]:     return dist._verify_params_across_processes(process_group, tensors, logger)
[rank1]: NotImplementedError: The operator 'c10d::allgather_' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 5811a8d7da873dd699ff6687092c225caffcf1bb. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.
E1128 18:56:59.854000 39080 site-packages/torch/distributed/elastic/multiprocessing/api.py:882] failed (exitcode: 1) local_rank: 0 (pid: 39085) of binary: /opt/miniconda3/bin/python3.13
Traceback (most recent call last):
  File "/opt/miniconda3/bin/torchrun", line 8, in <module>
    sys.exit(main())
             ~~~~^^
  File "/opt/miniconda3/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
    return f(*args, **kwargs)
  File "/opt/miniconda3/lib/python3.13/site-packages/torch/distributed/run.py", line 936, in main
    run(args)
    ~~~^^^^^^
  File "/opt/miniconda3/lib/python3.13/site-packages/torch/distributed/run.py", line 927, in run
    elastic_launch(
    ~~~~~~~~~~~~~~~
        config=config,
        ~~~~~~~~~~~~~~
        entrypoint=cmd,
        ~~~~~~~~~~~~~~~
    )(*cmd_args)
    ~^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.13/site-packages/torch/distributed/launcher/api.py", line 156, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/miniconda3/lib/python3.13/site-packages/torch/distributed/launcher/api.py", line 293, in launch_agent
    raise ChildFailedError(
    ...<2 lines>...
    )
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-11-28_18:56:59
  host      : 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 39086)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2025-11-28_18:56:59
  host      : 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 39087)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2025-11-28_18:56:59
  host      : 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 39088)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-11-28_18:56:59
  host      : 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 39085)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
